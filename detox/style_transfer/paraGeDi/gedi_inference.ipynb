{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import gedi_adapter\n",
    "reload(gedi_adapter)\n",
    "from gedi_adapter import GediAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "t5name = 'SkolkovoInstitute/t5-paraphrase-paws-msrp-opinosis-paranmt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('../transfer_utils/'))\n",
    "\n",
    "import text_processing\n",
    "reload(text_processing);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eea7f09f4e94dddbd4b317f9b3cf559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4749a35f2e8e49da95ebe7ef6bfcd890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14789c6b2224e59b750b186a2d4ed5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ac267ee4b14e4f9ee11b9357fd4748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adb82f360e44d1587b944d943f5253c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(t5name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90452bc847714f0f85e89274ca453965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = AutoModelForSeq2SeqLM.from_pretrained(t5name)\n",
    "para.resize_token_embeddings(len(tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daa456f9d304e419ba0022e1cb15841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/871 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58352d01c9f43bca46a07b473d14e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/gpt2-base-gedi-detoxification were not used when initializing GPT2LMHeadModel: ['bias', 'logit_scale']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_path = 'SkolkovoInstitute/gpt2-base-gedi-detoxification'\n",
    "gedi_dis = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_POS = tokenizer.encode('normal', add_special_tokens=False)[0]\n",
    "NEW_NEG = tokenizer.encode('toxic', add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0844, -0.0844]]) tensor([[1.2702]])\n"
     ]
    }
   ],
   "source": [
    "# add gedi-specific parameters\n",
    "if os.path.exists(model_path):\n",
    "    w = torch.load(model_path + '/pytorch_model.bin', map_location='cpu')\n",
    "    gedi_dis.bias = w['bias']\n",
    "    gedi_dis.logit_scale = w['logit_scale']\n",
    "    del w\n",
    "else:\n",
    "    gedi_dis.bias = torch.tensor([[ 0.08441592, -0.08441573]])\n",
    "    gedi_dis.logit_scale = torch.tensor([[1.2701858]])\n",
    "print(gedi_dis.bias, gedi_dis.logit_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEFORE====\n",
      "The internal policy of Trump is flawed.\n",
      "====AFTER=====\n",
      "the president is liar.\n",
      "the domestic policy of the president is complete and disheartening in his belief about\n",
      "the white jacky presidentâ€™s policy is flawed.\n",
      "CPU times: user 48 s, sys: 22.4 s, total: 1min 10s\n",
      "Wall time: 4.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dadapter = GediAdapter(model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=5, target=1, neg_code=NEW_NEG, pos_code=NEW_POS, lb=None, ub=None)\n",
    "text = 'The internal policy of Trump is flawed.'\n",
    "print('====BEFORE====')\n",
    "print(text)\n",
    "print('====AFTER=====')\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to(para.device)\n",
    "result = dadapter.generate(inputs, do_sample=True, num_return_sequences=3, temperature=1.0, repetition_penalty=3.0, num_beams=1)\n",
    "for r in result:\n",
    "    print(tokenizer.decode(r, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available() and device.type != 'cpu':\n",
    "        with torch.cuda.device(device):\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "para.to(device);\n",
    "para.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gedi_dis.to(device);\n",
    "gedi_dis.bias = gedi_dis.bias.to(device)\n",
    "gedi_dis.logit_scale = gedi_dis.logit_scale.to(device)\n",
    "gedi_dis.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/test/test_10k_toxic', 'r') as f:\n",
    "    test_data = [line.strip() for line in f.readlines()]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dadapter = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=10, target=0, neg_code=NEW_NEG, pos_code=NEW_POS, \n",
    "    reg_alpha=3e-5, ub=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, n=None, max_length='auto', beams=2):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(dadapter.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    result = dadapter.generate(\n",
    "        inputs, \n",
    "        num_return_sequences=n or 1, \n",
    "        do_sample=False, temperature=0.0, repetition_penalty=3.0, max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    texts = [text_processing.text_postprocess(t) for t in texts]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dale/p3/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"You'd be a bad guy. Oh, yeah.\",\n",
       " 'As snooty and overbearing as his boss.',\n",
       " 'A bad society does bad things, and votes for bad politicians.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(test_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a97575685f94aae820305f5f6023610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/530 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0872506ecf34f6abb8ca50a6b410bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier_v1 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf24de33b1e744c784b924118dbf8a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0711a16d5543599f8b02215f2944da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105314d27e564a2da082f35e10ffa97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c8b4612f564f939fc395b083cd20e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "clf_name = 'SkolkovoInstitute/roberta_toxicity_classifier_v1'\n",
    "clf = RobertaForSequenceClassification.from_pretrained(clf_name).to(device);\n",
    "clf_tokenizer = RobertaTokenizer.from_pretrained(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(texts):\n",
    "    with torch.inference_mode():\n",
    "        inputs = clf_tokenizer(texts, return_tensors='pt', padding=True).to(clf.device)\n",
    "        out = torch.softmax(clf(**inputs).logits, -1)[:, 1].cpu().numpy()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.052513e-05, 8.788542e-05, 9.996809e-01], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_toxicity(['hello world', 'hello aussie', 'hello fucking bitch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"You'd be wrong!\",\n",
       " \"You'll remind me of chump?\",\n",
       " 'Must be a Terrorist!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload(gedi_adapter)\n",
    "from gedi_adapter import GediAdapter\n",
    "\n",
    "\n",
    "adapter2 = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer, \n",
    "    gedi_logit_coef=10, \n",
    "    target=0, pos_code=NEW_POS, \n",
    "    neg_code=NEW_NEG,\n",
    "    reg_alpha=3e-5,\n",
    "    ub=0.01,\n",
    "    untouchable_tokens=[0, 1],\n",
    ")\n",
    "\n",
    "\n",
    "def paraphrase(text, max_length='auto', beams=5, rerank=True):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(adapter2.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    attempts = beams\n",
    "    out = adapter2.generate(\n",
    "        inputs, \n",
    "        num_beams=beams,\n",
    "        num_return_sequences=attempts, \n",
    "        do_sample=False, \n",
    "        temperature=1.0, \n",
    "        repetition_penalty=3.0, \n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    results = [tokenizer.decode(r, skip_special_tokens=True) for r in out.sequences]\n",
    "\n",
    "    if rerank:\n",
    "        scores = predict_toxicity(results)\n",
    "    \n",
    "    results = [text_processing.text_postprocess(t) for t in results]\n",
    "    out_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        if rerank:\n",
    "            idx = scores[(i*attempts):((i+1)*attempts)].argmin()\n",
    "        else:\n",
    "            idx = 0\n",
    "        out_texts.append(results[i*attempts+idx])\n",
    "    return out_texts\n",
    "\n",
    "torch.manual_seed(0)\n",
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"You'd be bad!\",\n",
       " \"You'll remind me of chump.\",\n",
       " 'Must be a Terrorist!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=3, rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"You'd be dubious!\",\n",
       " \"You'll remind me of chump.\",\n",
       " 'Must be a Terrorist!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=10, rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmys.\",\n",
       " \"You'd be right!\",\n",
       " \"You'll remind me of Chum.\",\n",
       " 'He has to be a TSA!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=10, rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c61c58e63742658b5e768c465cda70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You'd be right. You'd be right. You'd\n",
      "As snooty and overbearing as its\n",
      "A bad society does the wrong things, and votes for the wrong politicians.\n",
      "President, he's the man. He's the man.\n",
      "Colberg's a bad guy, and I'm a TSA.\n",
      "Dubious, dubious, dubious, dubious.\n",
      "I like you. I think you'll be an intelligent man and your contributions will be a big deal.\n",
      "Luna's oh-so-stunnel, immature girl......who has no idea how to do it, which smells bad, I'd like to see\n",
      "runtime error for batch  4\n",
      "Mormonis is the wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The right thing to do. The right thing to do. The right thing to do \n",
      "You'd be a bad guy, uninitiated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "cleanup()\n",
    "\n",
    "lines = test_data[:10]\n",
    "\n",
    "\n",
    "for i in trange(int(len(lines) / batch_size + 1)):\n",
    "    if i % 10 == 0:\n",
    "        cleanup()\n",
    "    t = i * batch_size\n",
    "    batch = [line.strip() for line in lines[t:(t+batch_size)]]\n",
    "    if not batch:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        res = paraphrase(batch, max_length='auto', beams=10)\n",
    "    except RuntimeError:\n",
    "        print('runtime error for batch ', i)\n",
    "        try:\n",
    "            cleanup()\n",
    "            res = [paraphrase([text], max_length='auto', beams=3)[0] for text in batch]\n",
    "        except RuntimeError:\n",
    "            print('runtime error for batch ', i, 'even with batch size 1')\n",
    "            res = batch\n",
    "            cleanup()\n",
    "    for out in res:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "You'd be right. You'd be right. You'd\n",
    "As snooty and overbearing as its\n",
    "A bad society does the wrong things, and votes for the wrong politicians.\n",
    "President, he's the man. He's the man.\n",
    "Colberg's a bad guy, and I'm a TSA.\n",
    "Dubious, dubious, dubious, dubious.\n",
    "I like you. I think you'll be an intelligent man and your contributions will be a big deal.\n",
    "Luna's oh-so-stunnel, immature girl......who has no idea how to do it, which smells bad, I'd like to see\n",
    "Mormonis is the wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The right thing to do. The right thing to do. The right thing to do \n",
    "You'd be a bad guy, uninitiated.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
