{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(path_raw + 'clean_comentarios_facebook.csv', sep=',', engine='python')\n",
    "data2 = pd.read_csv(path_raw + 'labeled_corpus_6K.txt', sep=';\\\\|\\\\|;', engine='python', names=['id','twt', 'label'])\n",
    "data3 = pd.read_csv(path_raw + 'sp_tweets.csv', sep=',', engine='python')\n",
    "data4 = pd.read_csv(path_raw + 'train_dev_es_merged.tsv', sep='\\t')\n",
    "data5 = pd.read_csv(path_raw +'hascosva_2022_anonymized.tsv', sep='\\t')\n",
    "\n",
    "#hateval2019\n",
    "data6 = pd.read_csv(path_raw +'hateval2019_es_train.csv', sep=',', engine='python')\n",
    "data7 = pd.read_csv(path_raw +'hateval2019_es_test.csv', sep=',', engine='python')\n",
    "data8 = pd.read_csv(path_raw +'hateval2019_es_dev.csv', sep=',', engine='python')\n",
    "\n",
    "data9 = pd.read_csv(path_raw +'corpusTradLabeled.csv', sep=',', engine='python')\n",
    "data10 = pd.read_csv(path_raw +'jigsaw-toxic-comment-train-google-es-cleaned.csv', sep=',', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicData = []\n",
    "normalData = []\n",
    "\n",
    "normalData.extend(data1.loc[data1['Category'] == 0, 'Text'].tolist())\n",
    "toxicData.extend(data1.loc[data1['Category'] == 1, 'Text'].tolist())\n",
    "\n",
    "normalData.extend(data2.loc[data2['label'] == 0, 'twt'].tolist())\n",
    "toxicData.extend(data2.loc[data2['label'] == 1, 'twt'].tolist())\n",
    "\n",
    "toxicData.extend(data3['tweet'].tolist())\n",
    "\n",
    "normalData.extend(data4.loc[data4['HS'] == 0, 'text'].tolist())\n",
    "toxicData.extend(data4.loc[data4['HS'] == 1, 'text'].tolist())\n",
    "\n",
    "normalData.extend(data5.loc[data5['label'] == 0, 'text'].tolist())\n",
    "toxicData.extend(data5.loc[data5['label'] == 1, 'text'].tolist())\n",
    "\n",
    "normalData.extend(data6.loc[data6['HS'] == 0, 'text'].tolist())\n",
    "normalData.extend(data7.loc[data7['HS'] == 0, 'text'].tolist())\n",
    "normalData.extend(data8.loc[data8['HS'] == 0, 'text'].tolist())\n",
    "toxicData.extend(data6.loc[data6['HS'] == 1, 'text'].tolist())\n",
    "toxicData.extend(data7.loc[data7['HS'] == 1, 'text'].tolist())\n",
    "toxicData.extend(data8.loc[data8['HS'] == 1, 'text'].tolist())\n",
    "\n",
    "normalData.extend(data9.loc[data9['label2'] == 0, 'text'].tolist())\n",
    "toxicData.extend(data9.loc[data9['label2'] == 1, 'text'].tolist())\n",
    "\n",
    "normalData.extend(data10.loc[data10['toxic'] == 0, 'comment_text'].tolist())\n",
    "toxicData.extend(data10.loc[data10['toxic'] == 1, 'comment_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = pd.DataFrame({'twt': toxicData, 'label': 1})\n",
    "normal = pd.DataFrame({'twt': normalData, 'label': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.concat([toxic, normal], ignore_index=True)\n",
    "df[\"label\"] =  df[\"label\"].astype(bool)\n",
    "df[\"twt\"] =  df[\"twt\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import preprocessor as p\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "def clean_text(text):\n",
    "    # Clean with tweet preprocessor: remove URLs, Hashtags, Mentions, Emojis, Smileys\n",
    "    text = p.clean(text) \n",
    "\n",
    "    # Convert text to lowercase, remove punctuation, characters with numbers, and newlines\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twt_cleaned'] = df['twt'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>twt</th>\n",
       "      <th>label</th>\n",
       "      <th>twt_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>y pensar ganar respetar ver aborrencimiento de...</td>\n",
       "      <td>True</td>\n",
       "      <td>y pensar ganar respetar ver aborrencimiento de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a a holgar ver hombre necesitar coser obvio ne...</td>\n",
       "      <td>True</td>\n",
       "      <td>a a holgar ver hombre necesitar coser obvio ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>drogar mocoso q empezar a hablar reportaje met...</td>\n",
       "      <td>True</td>\n",
       "      <td>drogar mocoso q empezar a hablar reportaje met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mujer feminazi destruir año verdadero luchar f...</td>\n",
       "      <td>True</td>\n",
       "      <td>mujer feminazi destruir ao verdadero luchar fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>feminista feminoide</td>\n",
       "      <td>True</td>\n",
       "      <td>feminista feminoide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444644</th>\n",
       "      <td>447146</td>\n",
       "      <td>\"\\n\\n == Homosexualidad - Discrepancia ==\\n\\n ...</td>\n",
       "      <td>False</td>\n",
       "      <td>homosexualidad  discrepancia este artculo esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444645</th>\n",
       "      <td>447147</td>\n",
       "      <td>:: ¿Consenso para arruinar Wikipedia? Creo que...</td>\n",
       "      <td>False</td>\n",
       "      <td>consenso para arruinar wikipedia creo que eso ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444646</th>\n",
       "      <td>447148</td>\n",
       "      <td>Acabo de descubrir otra lista: la lista de com...</td>\n",
       "      <td>False</td>\n",
       "      <td>acabo de descubrir otra lista la lista de comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444647</th>\n",
       "      <td>447149</td>\n",
       "      <td>\"\\n He estado leyendo esta página de discusión...</td>\n",
       "      <td>False</td>\n",
       "      <td>he estado leyendo esta pgina de discusin y me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444648</th>\n",
       "      <td>447150</td>\n",
       "      <td>== ¡Qué vergüenza para todos! ==\\n\\n Quieres h...</td>\n",
       "      <td>False</td>\n",
       "      <td>qu vergenza para todos quieres hablar de gays ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444649 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                                twt  label  \\\n",
       "0            0  y pensar ganar respetar ver aborrencimiento de...   True   \n",
       "1            1  a a holgar ver hombre necesitar coser obvio ne...   True   \n",
       "2            2  drogar mocoso q empezar a hablar reportaje met...   True   \n",
       "3            3  mujer feminazi destruir año verdadero luchar f...   True   \n",
       "4            4                                feminista feminoide   True   \n",
       "...        ...                                                ...    ...   \n",
       "444644  447146  \"\\n\\n == Homosexualidad - Discrepancia ==\\n\\n ...  False   \n",
       "444645  447147  :: ¿Consenso para arruinar Wikipedia? Creo que...  False   \n",
       "444646  447148  Acabo de descubrir otra lista: la lista de com...  False   \n",
       "444647  447149  \"\\n He estado leyendo esta página de discusión...  False   \n",
       "444648  447150  == ¡Qué vergüenza para todos! ==\\n\\n Quieres h...  False   \n",
       "\n",
       "                                              twt_cleaned  \n",
       "0       y pensar ganar respetar ver aborrencimiento de...  \n",
       "1       a a holgar ver hombre necesitar coser obvio ne...  \n",
       "2       drogar mocoso q empezar a hablar reportaje met...  \n",
       "3       mujer feminazi destruir ao verdadero luchar fe...  \n",
       "4                                     feminista feminoide  \n",
       "...                                                   ...  \n",
       "444644  homosexualidad  discrepancia este artculo esta...  \n",
       "444645  consenso para arruinar wikipedia creo que eso ...  \n",
       "444646  acabo de descubrir otra lista la lista de comp...  \n",
       "444647  he estado leyendo esta pgina de discusin y me ...  \n",
       "444648  qu vergenza para todos quieres hablar de gays ...  \n",
       "\n",
       "[444649 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.replace('', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(data, prefix, test_size=0.2, dev_size=0.1, random_state=42):\n",
    "    \n",
    "    train_data, temp_data = train_test_split(data, test_size=(test_size + dev_size), random_state=random_state)\n",
    "    \n",
    "    test_data, dev_data = train_test_split(temp_data, test_size=(dev_size / (test_size + dev_size)), random_state=random_state)\n",
    "    \n",
    "    train_data.to_csv(f'./processed/train_{prefix}', index=False,header=False)\n",
    "    test_data.to_csv(f'./processed/test_{prefix}', index=False,header=False)\n",
    "    dev_data.to_csv(f'./processed/dev_{prefix}', index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_save(df[df['label']==1].twt_cleaned, 'toxic')\n",
    "split_and_save(df[df['label']==0].twt_cleaned, 'normal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
