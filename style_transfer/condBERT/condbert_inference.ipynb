{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def add_sys_path(p):\n",
    "    p = os.path.abspath(p)\n",
    "    print(p)\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import condbert\n",
    "reload(condbert)\n",
    "from condbert import CondBertRewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load vocabularies for spans detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_root = 'vocab_condbert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_root + \"negative-words.txt\", \"r\") as f:\n",
    "    s = f.readlines()\n",
    "negative_words = list(map(lambda x: x[:-1], s))\n",
    "with open(vocab_root + \"toxic_words.txt\", \"r\") as f:\n",
    "    ss = f.readlines()\n",
    "negative_words += list(map(lambda x: x[:-1], ss))\n",
    "\n",
    "with open(vocab_root + \"positive-words.txt\", \"r\") as f:\n",
    "    s = f.readlines()\n",
    "positive_words = list(map(lambda x: x[:-1], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(vocab_root + 'word2coef.pkl', 'rb') as f:\n",
    "    word2coef = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_toxicities = []\n",
    "with open(vocab_root + 'token_toxicities.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        token_toxicities.append(float(line))\n",
    "token_toxicities = np.array(token_toxicities)\n",
    "token_toxicities = np.maximum(0, np.log(1/(1/token_toxicities-1)))   # log odds ratio\n",
    "\n",
    "# discourage meaningless tokens\n",
    "for tok in ['.', ',', '-']:\n",
    "    token_toxicities[tokenizer.encode(tok)][1] = 3\n",
    "\n",
    "for tok in ['you']:\n",
    "    token_toxicities[tokenizer.encode(tok)][1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(condbert)\n",
    "from condbert import CondBertRewriter\n",
    "\n",
    "editor = CondBertRewriter(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    neg_words=negative_words,\n",
    "    pos_words=positive_words,\n",
    "    word2coef=word2coef,\n",
    "    token_toxicities=token_toxicities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! jacintooo . . . ! , ! ! ! te voy a sacar la mi onda . . . ! ! !\n"
     ]
    }
   ],
   "source": [
    "print(editor.translate(\"! Jacintooo...!, !!!te voy a sacar la mierda...!!!\", prnt=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = CondBertRewriter(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    neg_words=negative_words,\n",
    "    pos_words=positive_words,\n",
    "    word2coef=word2coef,\n",
    "    token_toxicities=token_toxicities,\n",
    "    predictor=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiword import masked_token_predictor_bert\n",
    "reload(masked_token_predictor_bert)\n",
    "from multiword.masked_token_predictor_bert import MaskedTokenPredictorBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! jacintooo . . . ! , ! ! ! te voy a sacar la mi . . . ! ! !\n"
     ]
    }
   ],
   "source": [
    "predictor = MaskedTokenPredictorBert(model, tokenizer, max_len=250, device=device, label=0, contrast_penalty=0.0)\n",
    "editor.predictor = predictor\n",
    "\n",
    "def adjust_logits(logits, label):\n",
    "    return logits - editor.token_toxicities * 3\n",
    "\n",
    "predictor.logits_postprocessor = adjust_logits\n",
    "\n",
    "print(editor.replacement_loop('! Jacintooo...!, !!!te voy a sacar la mierda...!!!', verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! jacintooo . . . ! , ! ! ! te voy a sacar la mi . . . ! ! !\n",
      "CPU times: user 404 ms, sys: 314 μs, total: 404 ms\n",
      "Wall time: 404 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(editor.replacement_loop('! Jacintooo...!, !!!te voy a sacar la mierda...!!!', verbose=False, n_units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! jacintooo . . . ! , ! ! ! te voy a sacar la mi . . . ! ! !\n",
      "CPU times: user 3.85 s, sys: 328 ms, total: 4.18 s\n",
      "Wall time: 4.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(editor.replacement_loop('! Jacintooo...!, !!!te voy a sacar la mierda...!!!', verbose=False, n_units=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! jacintooo . . . ! , ! ! ! te voy a sacar la mi . . . ! ! !\n",
      "CPU times: user 4.07 s, sys: 328 ms, total: 4.4 s\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(editor.replacement_loop('! Jacintooo...!, !!!te voy a sacar la mierda...!!!', verbose=False, n_units=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import choosers\n",
    "reload(choosers)\n",
    "from choosers import EmbeddingSimilarityChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataTest = pd.read_csv('../../data/processed/test.txt', sep='\\t', header=None, names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.o de poco vamos a tener malaria por todos estos congos sudacas de africa sudacas de africaaaaaaaaaaaaaaa',\n",
       " '@ milagrosmbarros @ agusq _ 99 jajajajajajaja callate que ni labial tenias zorra , no [UNK]',\n",
       " 'te subis a un bondi y esta todo cerrado , abran una venta manga de bichos refugiados',\n",
       " 'la \" marcha negra \" acelera el paso : mas de 40 . 000 subsaharianos esperan en marruecos para entrar ilegalmente en espana https : / / t . co / 0mhgjnwjoz ¡ ¡ okupa monclovita , . . . ¿ ¿ tienes pensado donde cobijar a toda esta gente . . . ? prepara barracas en moncloa , creo que hay terreno . . . ! !',\n",
       " '@ mariooovt esta pepe zorra callate',\n",
       " 'el estado del bienestar para quien ? para inmigrantes ( mantero , violadores y demas chusma que estais trayendo ) pagas y se que tenemos que pagar los demas ? para cuando el bienestar de los espanoles , seguridad en el trabajo de policia y guardia civil , trabajo y ayuda xa parados . . ? https : / / t . co / yu4cczmpga',\n",
       " '@ lamambanegra17 @ breendlp @ chumeltorres @ juandedios _ p @ kimberlyloaiza _ ena estupida , pendeja y tu eres buena para ser [UNK]',\n",
       " '@ rubenssambueza eres una basura de persona , lo cual no se sorprende porque eres sudaca , y asi en los tercermundistas',\n",
       " 'muerete zorra que el no puedo ver monstruos s . a . — callate gilipollas https : / / t . co / bza1mguble',\n",
       " '@ ludmi526 callate zorra , me haces acordar a nuestro soldado caido @ vickydeamicis',\n",
       " 'ensenan al pueblo espanol a amedrentarse frente a un enemigo , yo enseno a mis hijos a saber defenderse , sea mantero , presidente del gobierno o saltador general de la valla de melilla ! !',\n",
       " '@ jubarrientoss callate anta que vos sos complice',\n",
       " 'me maman esos casos en los que el hombre es el enculado y la mujer es bien perra , es como ver el otro lado de la moneda [UNK]',\n",
       " '@ joaoftigoa hija de puta callate ajsjajakajajajsj',\n",
       " \"la misma zagala al zagal que le acompana : ' tu eres en de postureo . que tienes 16 anos y no has olido ni una polla ' . esta nina es una mina y por lo que parece , experta en pollas .\",\n",
       " '@ kelvexm @ jessiijg @ zdeprooz _ @ jenniferelm1 tu eres mi oveja puta',\n",
       " 'con todo lo que esta entrando por la valla de ceuta y melilla , el recuerdo no llaman a la puerta el piden por favor entrar . vienen a invadir e intentar destruirnos y aniquilar nuestra sociedad ayudado por los pijos progres # polisionunutiacatlaska https : / / t . co / ywwapoz2pw',\n",
       " '@ ainoatundidor3 callate , q tu si q t presenta . pedazo d perra',\n",
       " '@ osorio _ nicoo callate perra , te contestas a las 11 de la noche',\n",
       " \"@ yasmicabaleiro jajajajaja callate zorra vieja jajaja , ' aprobe una , demasiado jajajaja\",\n",
       " 'alguien el calculado cuantos subsaharianos puede absorber el gremio de \" gorrillas \" y \" manteros \" ?',\n",
       " '@ carmeladp07 por favor de prometas eso [UNK] tu eres perra .',\n",
       " '@ carmebarcelo @ 3gerardpique una polla de 30 cm te hace falta para estar ocupada y no decir tonterias',\n",
       " '# espana , el lugar donde gente que ha entrado # ilegalmente en el pais , a veces utilizando la fuerza , y que realizan actividades de venta ilegal , sin licencias , sin papeles de ningun tipo y saltandose todas las leyes , ¡ tienen un sindicato ! . mientras , los # autonomos son machacados .',\n",
       " '@ perradesatan @ iamjoseam pues tengo todo el dia del seno libre . y como tu eres una perra del angel caido se pensado que podiamos hacerlo frente a una iglesia .',\n",
       " 'mi hermana la mando lo de si fonsi y lo hice bien , jaja en tu cara zorra [UNK]',\n",
       " '@ cutiemoon _ callate , eres perfecto maldita sea no el hagas caso a la perra zorra de titan , que te valga puta verga lo que diga , aaaaaaaah',\n",
       " 'que guarra tienes que ser en el catre — cuando quiero con quien quiero https : / / t . co / nbuse7ejjb',\n",
       " 'las viejas de antes , puteaban bonito para ganar dinero . las de hoy acusan de acoso o violacion a algun pobre y buen devio .',\n",
       " '@ elisacarrio callate hija de puta gorda falopera',\n",
       " 'no era machismo pero - creo que las mujeres son ases que los vevios . lo que hay que cree eliot .',\n",
       " '@ beeluarzola hija de re mil puta mereces un tiro en la concha puta de miorda ! ! ! ! !',\n",
       " '@palopascalee paloma cállate la boca que sos de flaca hija de perra una envidia',\n",
       " \"@UnicornHappy13 CALLATE PERRA!! :'v\",\n",
       " 'los venezolanos llamamos por anos : ladrones a los chinos tacanos a los arabes mala gente a los colombianos indios y feos a los peruanos vagos a los panamenos y venimos a indignarnos cuando nos llaman venecos en otro lado . xenofobico el venezolano , para nada .',\n",
       " 'campo de refugiados en espana [UNK] q gobierno tendra los santos cohones de poner fin si o si a este desproposito ? # campoderefugiados # espana https : / / t . co / ejgckanzrz',\n",
       " 'la policia britanica permitio la violacion de 1 . 000 ninas pobres por parte de arabes https : / / t . co / 2uricnljo9 via @ laotracara _ co',\n",
       " '@William43591799 Tu eres una perra',\n",
       " '@ ulrikvc callate puta en aleman',\n",
       " '@ ricardo _ peralta callate esta creo que aun sigo ebria [UNK]',\n",
       " 'nosotros como tu dices seremos gentuza , tu eres una gran hija de puta , ademas con pinta de zorra @ lafallaras',\n",
       " 'a esto anadidle el aguantar podemitas y el tener que absorber cantidades enos de subsaharianos . los partidos seguirian jodiendo igual . ¡ sigan mandado ! https : / / t . co / dcsks7gspi',\n",
       " '- ¿ cual es tu problema ? - mi novia es tremenda perra ! ! ! https : / / t . co / 2urgcrqisc',\n",
       " 'l # informateenholachile los reseponsables del ingreso al pais de tantos inmigrantes deben hacerse cargo del los problemas de salud de ellos porque actualmente han venido a aumentar el el problema de atencion medica de chilenos , ocupando las pocas horas de atencion y recursos',\n",
       " '@ brotherdroiid @ snowknight9 pero si tu eres igual de perra y rata haci que no vengas a hablar maldita dejenerada > : v',\n",
       " '.je de tuitear hija de puta escucha los argumentos que luego te vas a quedar callada fujimierda del carajo https : / / t . co / np9rav5aui',\n",
       " 'quiero que el pibe me diga “ tu eres mi puta la mejor que que “ jaaja ahre',\n",
       " '@ mishelleha00ya callate ena mara , la corrieron a tu papa por tus pendejadas y sigues y sigues en tu puta vida entenderas la grandeza de este club',\n",
       " '@ alan15537141 callate que despues que andas abrazando puta loca jajaja']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "for twt in dataTest['text']:\n",
    "    test.append(editor.translate(twt, prnt=False))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['el diario de poco vamos a tener malaria por todos estos negro sudacas de africa sudacas de africaaaaaaaaaaaaaaa',\n",
       " '@ milagrosmbarros @ agusq _ 99 jajajajajajaja callate que ni labial tenias zorra , yo [UNK]',\n",
       " 'te subis a un bondi y esta todo cerrado , abran una semana manga de bichos refugiados',\n",
       " 'la \" marcha negra \" acelera el paso : mas de 40 . 000 subsaharianos esperan en el para entrar ilegalmente en espana https : / / t . co / 0mhgjnwjoz ¡ ¡ okupa monclovita , . . . ¿ que no tienes pensado donde cobijar a toda esta gente . . . ? prepara barracas en moncloa , creo que hay terreno . . . ! !',\n",
       " '@ mariooovt esta ? @ @ zorra callate',\n",
       " 'el estado del bienestar para quien ? para inmigrantes ( mantero , violadores y demas chusma que estais trayendo ) pagas y espasa que tenemos que pagar los demas ? para cuando el bienestar de los espanoles , seguridad en el trabajo de policia y guardia civil , trabajo y ayuda xa parados . . ? https : / / t . co / yu4cczmpga',\n",
       " '@ lamambanegra17 @ breendlp @ chumeltorres @ juandedios _ p @ kimberlyloaiza _ p @ estupida , pendeja y tu eres buena para ser [UNK]',\n",
       " '@ rubenssambueza eres una basura de persona , lo cual no me sorprende porque eres sudaca , y asi en los tercermundistas',\n",
       " 'muerete zorra que a no puedo ver monstruos s . a . — callate gilipollas https : / / t . co / bza1mguble',\n",
       " '@ ludmi526 callate zorra , me haces acordar a nuestro soldado caido @ vickydeamicis',\n",
       " 'ensenan al pueblo espanol a amedrentarse frente a un enemigo , yo enseno a mis hijos a saber defenderse , sea mantero , presidente del gobierno o saltador general de la valla de melilla ! !',\n",
       " '@ jubarrientoss callate . com , que vos sos complice',\n",
       " 'me maman esos casos en los que el hombre es el enculado y la mujer es bien perra , es como ver el otro lado de la moneda [UNK]',\n",
       " '@ joaoftigoa hija de la vida callate ajsjajakajajajsj',\n",
       " \"la misma zagala al zagal que le acompana : ' tu eres que eso de postureo . que tienes 16 anos y no has olido ni una polla ' . esta nina es una mina y por lo que parece , experta en pollas .\",\n",
       " '@ kelvexm @ jessiijg @ zdeprooz _ @ jenniferelm1 tu eres mi oveja .',\n",
       " 'con todo lo que esta entrando por la valla de ceuta y melilla , le recuerdo no llaman a la puerta a piden por favor entrar . vienen a invadir e intentar destruirnos y aniquilar nuestra sociedad ayudado por los pijos progres # # # # https : / / t . co / ywwapoz2pw',\n",
       " '@ ainoatundidor3 callate , q tu si q t presenta . pedazo d perra',\n",
       " '@ osorio _ nicoo callate perra , me contestas a las 11 de la noche',\n",
       " '@ yasmicabaleiro jajajajaja callate zorra vieja jajaja , ya aprobe una , demasiado jajajaja',\n",
       " 'alguien a calculado cuantos subsaharianos puede absorber el gremio de \" gorrillas \" y \" manteros \" ?',\n",
       " '@ carmeladp07 por favor de prometas eso [UNK] tu eres perra .',\n",
       " '@ carmebarcelo @ 3gerardpique una polla de 30 cm te hace falta para estar ocupada y no decir tonterias',\n",
       " '# espana , el lugar donde gente que ha entrado # ilegalmente en el pais , a veces utilizando la fuerza , y que realizan actividades de los # ilegal , sin licencias , sin papeles de ningun tipo y saltandose todas las leyes , ¡ tienen un sin ! . mientras , los # autonomos son machacados .',\n",
       " '@ perradesatan @ iamjoseam pues tengo todo el dia del los luchas libre . y como tu eres una perra del angel caido he pensado que podiamos hacerlo frente a una iglesia .',\n",
       " 'mi hermana me mando lo dejes , fonsi y lo hice bien , jaja en tu cara zorra [UNK]',\n",
       " '@ cutiemoon _ callate , eres perfecto maldita sea no le hagas caso a la perra zorra de titan , que te valga puta verga lo que diga , aaaaaaaah',\n",
       " 'que guarra tienes que ser en el catre — cuando quiero con quien quiero https : / / t . co / nbuse7ejjb',\n",
       " 'las viejas de antes , puteaban bonito para ganar dinero . las de hoy acusan de acoso o violacion a algun pobre y buen ho .',\n",
       " '@ elisacarrio callate hija de la sierra gorda falopera',\n",
       " 'no es el machismo pero . no se creo que las mujeres sonidos que los ho . lo que hay que los hombre .',\n",
       " '@ beeluarzola hija de re mil puta mereces un tiro en la concha puta de mi ! ! ! ! !',\n",
       " '@palopascalee paloma cállate la boca que sos de flaca hija de perra una envidia',\n",
       " \"@UnicornHappy13 CALLATE PERRA!! :'v\",\n",
       " 'los venezolanos llamamos por anos : ladrones a los chinos tacanos a los arabes mala gente a los colombianos indios y feos a los peruanos vagos a los pan y venimos a indignarnos cuando nos llaman venecos en otro lado . xenofobico el venezolano , para nad .',\n",
       " 'campo de refugiados en espana [UNK] q gobierno tendra los santos cohones de poner fin si o si a este desproposito ? # campoderefugiados # espana https : / / t . co / ejgckanzrz',\n",
       " 'la policia britanica permitio la violacion de 1 . 000 ninas pobres a parte de arabes https : / / t . co / 2uricnljo9 via @ laotracara _ co',\n",
       " '@William43591799 Tu eres una perra',\n",
       " '@ ulrikvc callate put en aleman',\n",
       " '@ ricardo _ peralta callate . com . creo que aun sigo ebria [UNK]',\n",
       " 'nosotros como tu dices seremos gentuza , tu eres una gran hija de puta , ademas con pinta de zorra @ lafallaras',\n",
       " 'a esto anadidle el aguantar podemitas y el tener que absorber cantidades absurd de subsaharianos . los partidos seguirian jodiendo igual . ¡ sigan mandado ! https : / / t . co / dcsks7gspi',\n",
       " '- ¿ cual es tu noche ? - mi novia es tremenda perra ! ! ! https : / / t . co / 2urgcrqisc',\n",
       " 'l # informateenholachile los reseponsables del ingreso al pais de tantos inmigrantes deben hacerse cargo del los problem de salud de ellos porque actualmente han venido a aumentar el los dos problem de atencion medica de chilenos , ocupando las pocas horas de atencion y recursos',\n",
       " '@ brotherdroiid @ snowknight9 pero si tu eres igual de perra y rata haci que no vengas a hablar maldita dejenerada > : v',\n",
       " 'el diario de tuitear hija de puta escucha los argumentos que luego te vas a quedar callada fujimierda del carajo https : / / t . co / np9rav5aui',\n",
       " 'quiero que el pibe me diga “ tu eres mi puta la mejor que no te “ jaaja ahre',\n",
       " '@ mishelleha00 ya callate , ya mar , ya corrieron a tu papa por tus pendejadas y sigues y sigues en tu puta vida entenderas la grandeza de este club',\n",
       " '@ alan15537141 callate que despues me andas abrazando puta loca jajaja']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = []\n",
    "for twt in dataTest['text']:\n",
    "    test2.append(editor.replacement_loop(twt, verbose=False))\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/inputcond1.txt', 'w') as input1, open('../../data/processed/inputcond2.txt', 'w') as input2:\n",
    "    for twt in test:\n",
    "        input1.writelines(f'{twt}\\n')\n",
    "    for twt in test2:\n",
    "        input2.writelines(f'{twt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.49s/it]\n",
      "Calculating BLEU similarity\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Calculating similarity by Wieting subword-embedding SIM model\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/metric/wieting_similarity/similarity_evaluator.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(self.model_path, **kw)\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 121.64it/s]\n",
      "Calculating CoLA acceptability stats\n",
      "models/cola\n",
      "checkpoint_best.pt\n",
      "models/cola/cola-bin\n",
      "2024-10-22 20:17:34 | INFO | fairseq.file_utils | loading archive file models/cola\n",
      "2024-10-22 20:17:34 | INFO | fairseq.file_utils | loading archive file models/cola/cola-bin\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/fairseq/checkpoint_utils.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\n",
      "2024-10-22 20:17:36 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
      "2024-10-22 20:17:36 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2it [00:02,  1.17s/it]                                                          \n",
      "| Model | ACC | SIM | FL | J | BLEU |\n",
      "\n",
      "| ----- | --- | --- | -- | - | ---- |\n",
      "\n",
      "output.txt|1.0000|0.7650|0.9592|0.7350|0.4735|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python metric/metric.py --inputs data/processed/inputcond1.txt --preds data/processed/output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.64s/it]\n",
      "Calculating BLEU similarity\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Calculating similarity by Wieting subword-embedding SIM model\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/metric/wieting_similarity/similarity_evaluator.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(self.model_path, **kw)\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 96.79it/s]\n",
      "Calculating CoLA acceptability stats\n",
      "models/cola\n",
      "checkpoint_best.pt\n",
      "models/cola/cola-bin\n",
      "2024-10-22 20:17:56 | INFO | fairseq.file_utils | loading archive file models/cola\n",
      "2024-10-22 20:17:56 | INFO | fairseq.file_utils | loading archive file models/cola/cola-bin\n",
      "/media/gabriel/Datos Linux/loncos/Text-Detoxification-in-Spanish/env/lib/python3.10/site-packages/fairseq/checkpoint_utils.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\n",
      "2024-10-22 20:17:58 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
      "2024-10-22 20:17:58 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2it [00:02,  1.15s/it]                                                          \n",
      "| Model | ACC | SIM | FL | J | BLEU |\n",
      "\n",
      "| ----- | --- | --- | -- | - | ---- |\n",
      "\n",
      "output.txt|1.0000|0.7652|0.9592|0.7351|0.4746|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python metric/metric.py --inputs data/processed/inputcond2.txt --preds data/processed/output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con ese cuerpo es toda una marraqueta\n",
      "['es'] -> ['es']\n",
      "['mar', '##ra', '##quet', '##a'] -> ['vida', '.']\n",
      "con ese cuerpo es toda una vida .\n"
     ]
    }
   ],
   "source": [
    "print(editor.translate(\"Con ese cuerpo es toda una marraqueta\", prnt=False))\n",
    "print(editor.replacement_loop(\"Con ese cuerpo es toda una marraqueta\", verbose=True, n_tokens=(1, 2), n_top=10))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esa chica . se tropezo dos veces desde que entro , es una petaca\n",
      "esa chica que no se tropezo dos veces desde que entro , es una petaca\n"
     ]
    }
   ],
   "source": [
    "print(editor.translate(\"Esa chica ya se tropezó dos veces desde que entró, es una petaca\", prnt=False))\n",
    "print(editor.replacement_loop(\"Esa chica ya se tropezó dos veces desde que entró, es una petaca\", verbose=False, n_tokens=(1, 2), n_top=10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el huanuli es era que va a poder cargar tantos sillares .\n",
      "el huanuli es que va a poder cargar tantos sillares .\n"
     ]
    }
   ],
   "source": [
    "print(editor.translate(\"El huañuli ese qué va a poder cargar tantos sillares.\", prnt=False))\n",
    "print(editor.replacement_loop(\"El huañuli ese qué va a poder cargar tantos sillares.\", verbose=False, n_tokens=(1, 2), n_top=10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
